import streamlit as st
import pymorphy2
import pandas as pd
import requests
import json

st.title("""ãƒ­ã‚·ã‚¢èªåŸå‹ãƒ¡ãƒ¼ã‚«ãƒ¼""")
st.header("""Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ½Ñ„Ğ¸Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ°""")

st.write("\n")
st.write("\n")
st.write("\n")

st.write("""
    ã“ã‚Œã¯ãƒ­ã‚·ã‚¢èªå­¦ç¿’è€…å‘ã‘ã®å˜èªã®åŸå‹æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã§ã™\n
    ç¿»è¨³ã¯ç„¡æ–™ã®å¤–éƒ¨ãƒªã‚½ãƒ¼ã‚¹ã®DeepLã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§æœˆã®åˆ©ç”¨ä¸Šé™ã‚’è¶…ãˆã‚‹ã¨æ­¢ã¾ã‚Šã¾ã™\n
    å€‹åˆ¥ã®å˜èªã®ç¿»è¨³ã¯ç›®ã‚‚å½“ã¦ã‚‰ã‚Œãªã„ã»ã©æ‚²æƒ¨ãªå‡ºæ¥ã ã£ãŸã®ã§ã‚„ã‚ã¾ã—ãŸ\n
    Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚ĞµÑ…, ĞºÑ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº\n
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ‡Ğ¸Ğº DeepL, ÑÑ‚Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚\n
    Ğ­Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ¸Ğ·-Ğ·Ğ° Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ ÑƒĞ¶Ğ°ÑĞ½Ñ‹Ğ¹ 
""")

st.write("\n")
st.write("\n")
st.write("\n")

search_words = st.text_input("æ¤œç´¢ (ĞŸĞ¾Ğ¸ÑĞº ÑĞ»Ğ¾Ğ²)", "æ¤œç´¢ã—ãŸã„å˜èªã‚’å…¥ã‚Œã¦ãã ã•ã„ ĞĞ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ ÑĞ»Ğ¾Ğ²Ğ°")


with open("auth.json") as json_open:
    json_load = json.load(json_open)

API_KEY = "3237dd9b-f637-dabd-af60-64ad5eae28da:fx"
# API_KEY = json_load["api_key"]


target_lang = "ja"

def translation(words):

    params = {
                'auth_key' : API_KEY,
                'text' : words,
                "target_lang": target_lang
            }
    request = requests.post("https://api-free.deepl.com/v2/translate", data=params)
    result = request.json()

    return result['translations'][0]['text']

parts_of_speech ={

    "NOUN" : "åè©ã€€Ğ¡ÑƒÑ‰",
    "ADJF" : "å½¢å®¹è©ã€€ĞŸ.ĞŸÑ€Ğ¸",
    "ADJS" : "å½¢å®¹è©çŸ­ç¸®å½¢ã€€Ğš.ĞŸÑ€Ğ¸",
    "COMP" : "æ¯”è¼ƒè©ã€€ĞšĞ¾Ğ¼Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¸Ğ²",
    "VERB" : "å‹•è©ã€€Ğ“Ğ»Ğ°Ğ³Ğ¾Ğ»",
    "INFN" : "ä¸å®šå½¢ã€€Ğ˜Ğ½Ñ„Ğ¸Ğ½Ğ¸Ñ‚Ğ¸Ğ²",
    "PRTF" : "å½¢å‹•è©ã€€ĞŸ.ĞŸÑ€Ğ¸Ñ‡Ğ°ÑÑ‚Ğ¸Ğµ",
    "PRTS" : "çŸ­ç¸®å½¢å‹•è©ã€€Ğš.ĞŸÑ€Ğ¸Ñ‡Ğ°ÑÑ‚Ğ¸Ğµ",
    "GRND" : "å‰¯å‹•è©ã€€Ğ”ĞµĞµĞ¿Ñ€Ğ¸Ñ‡Ğ°ÑÑ‚Ğµ",
    "NUMR" : "æ•°è©ã€€Ğ§Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ",
    "ADVB" : "å‰¯è©ã€€ĞĞ°Ñ€ĞµÑ‡Ğ¸Ğµ",
    "NPRO" : "ä»£åè©ã€€ĞœĞµÑÑ‚",
    "PRED" : "å¦å®šä»£åè©ã€€ĞŸÑ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²",
    "PREP" : "å‰ç½®è©ã€€ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ³",
    "PREP" : "æ¥ç¶šè©ã€€Ğ¡Ğ¾ÑĞ·",
    "PRCL" : "ä¸å¤‰åŒ–è©ã€€Ğ§Ğ°ÑÑ‚Ğ¸Ñ†Ğ°",
    "INTJ" : "ĞœĞµĞ¶Ğ´Ğ¾Ğ¼ĞµÑ‚Ğ¸Ğµ",
    "<NA>" : " "
}

def lemmatize(words):

    words = words.replace(".", "").replace(",", "").replace("!", "").replace("?", "").replace("'", "").replace("\"", "")
    words = words.split(" ")
    lemma_words = []
    word_types = []
    #trans_words = []

    for word in words:
        
        analyzer = pymorphy2.MorphAnalyzer()
        lemma_word = analyzer.parse(word)[0].normal_form
        lemma_words.append(lemma_word)
        word_type = analyzer.parse(word)[0].tag.POS

        try:
            word_types.append(parts_of_speech[word_type])
        except:
            word_types.append("")
        # try:
        #     if lemma_word == "Ñ" or lemma_word == "Ğ¼ĞµĞ½Ñ" or lemma_word == "Ğ¼Ğ½Ğµ" or lemma_word == "Ğ¼Ğ½Ğ¾Ğ¹":
        #         trans_words.append("ã‚ãŸã—")
        #         continue
        #     trans_words.append(translation(lemma_word))
        # except:
        #     st.error('ä»Šæœˆã®ç¿»è¨³ä¸Šé™è¶…ãˆã¾ã—ãŸ (Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡ĞµĞ½ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ)', icon="ğŸš¨")      

    return words, lemma_words, word_types#, trans_words


lemma_df = pd.DataFrame(lemmatize(search_words), index=["å…ƒã®å˜èª (ĞŸÑ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ ÑĞ»Ğ¾Ğ²Ğ¾)", "åŸå‹ (Ğ˜Ğ½Ñ„Ğ¸Ğ½Ğ¸Ñ‚Ğ¸Ğ²)", "ã‚¿ã‚¤ãƒ— (Ğ§Ğ°ÑÑ‚ÑŒ Ñ€ĞµÑ‡Ğ¸)"]).T

try:
    translation_all = translation(search_words)
except:
    st.error('ä»Šæœˆã®ç¿»è¨³ä¸Šé™è¶…ãˆã¾ã—ãŸ (Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡ĞµĞ½ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ)', icon="ğŸš¨")      

st.write(lemma_df)

st.text_input(""" å…¨è¨³ (ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ğ²)""", translation_all)



def convert_df(df):
     # IMPORTANT: Cache the conversion to prevent computation on every rerun
     return df.to_csv().encode('utf-8')

csv = convert_df(lemma_df)

st.download_button(
     label="Download (Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒÑ€ÑŒÑ‚Ğ°Ñ‚)",
     data=csv,
     file_name='result.csv',
     mime='text/csv',
 )

st.caption("@shiki_yoshida")
st.caption("å•ã„åˆã‚ã›ãƒ»è¦æœ›ãªã©ã¯DMã§ãŠé¡˜ã„ã—ã¾ã™ã€€https://twitter.com/anya_ruski")
